# !pip install -q transformers einops accelerate langchain bitsandbytes huggingface_hub
# !pip install xformers
# !pip install pypdf


#!git config --global credential.helper store
#!huggingface-cli login
from huggingface_hub import notebook_login
notebook_login()


from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer
import transformers
import torch
import warnings
warnings.filterwarnings('ignore')


model="meta-llama/Llama-2-7b-chat-hf"
tokenizer=AutoTokenizer.from_pretrained(model)
pipeline=transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
    max_length=1000,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id
    )


llm=HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0})


prompt="What would be a good name for a company that makes colorful socks"


print(llm(prompt))


prompt="I want to open a restaurant for  Chinese food. Suggest me a fence name for this"


print(llm(prompt))


from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain


prompt_template=PromptTemplate(input_variables=["cuisine"],
                               template="I want to open a restaurant for {cuisine} food. Suggest a fency name for this")


input_prompt=prompt_template.format(cuisine="Italian")


print(input_prompt)


prompt_template=PromptTemplate(input_variables=["book_name"],
                               template="Privide me a concise summary of the book {book_name}")


input_prompt=prompt_template.format(book_name="Alchemist")


print(input_prompt)


prompt_template=PromptTemplate(input_variables=["book_name"],
                               template="Proivide me a concise summary of the book {book_name}")


chain = LLMChain(llm=llm, prompt=prompt_template, verbose=True)
response= chain.run("Alchemist")
print(response)


prompt_template=PromptTemplate(input_variables=["cuisine"],
                               template="I want to open a restaurant for {cuisine} food. Suggest me a fency name for this")


chain=LLMChain(llm=llm, prompt=prompt_template, verbose=True)


response=chain.run("Italian")
print(response)


from langchain.chains import SimpleSequentialChain


prompt_template_one=PromptTemplate(input_variables=["cuisine"],
                               template="I want to open a restaurant for {cuisine} food. Suggest me a fency name for this")



restaurant_name_chain=LLMChain(llm=llm, prompt=prompt_template_one)


prompt_template_two=PromptTemplate(input_variables=["restaurant_name"],
                                   template="""You are provided with a lot of restaurant names: {restaurant_name}, just pick the first
                                   restaurant name and suggest some menu items for the restaurant""")


restaurant_menu_items_chain=LLMChain(llm=llm, prompt=prompt_template_two)


chain=SimpleSequentialChain(chains=[restaurant_name_chain, restaurant_menu_items_chain])
chain.run("Italian")



from langchain.chains import SequentialChain


prompt_template_one=PromptTemplate(input_variables=["cuisine"],
                                   template="""I want to open a restaurant for {cuisine} food. Suggest me a fency
                                   name for this, please only provide me one restaurant name""")


restaurant_name_chain=LLMChain(llm=llm, prompt=prompt_template_one, output_key="restaurant_name")


prompt_template_two=PromptTemplate(input_variables=["restaurant_name"],
                                   template="""Suggest some menu items for the restaurant {restaurant_name}""")


restaurant_menu_items_chain=LLMChain(llm=llm, prompt=prompt_template_two, output_key="menu_items")


chain=SequentialChain(chains=[restaurant_name_chain, restaurant_menu_items_chain],
                      input_variables=["cuisine"],
                      output_variables=["restaurant_name", "menu_items"])


chain({"cuisine":"Italian"})


chain({"cuisine":"Paistani"})


prompt_template_name = PromptTemplate(
    input_variables =['product'],
    template = "What is a good name for a company that makes {product}"
)


chain = LLMChain(llm=llm,prompt=prompt_template_name)
name = chain.run("colorful socks")
print(name)


prompt_template_name = PromptTemplate(
    input_variables =['product'],
    template = "What would be  a good name for a company that makes {product}"
)



chain = LLMChain(llm=llm,prompt=prompt_template_name)



name = chain.run("Security Cameras")
print(name)


chain.memory


type(chain.memory)


from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory()

prompt_template_name = PromptTemplate(
    input_variables =['product'],
    template = "What is a good name for a company that makes {product}"
)

chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)
name = chain.run("colorful socks")
print(name)


name = chain.run("Drone Cameras")
prompt_template_name
print(name)


print(chain.memory.buffer)


from langchain.chains import ConversationChain

convo = ConversationChain(llm=llm)
print(convo.prompt.template)


convo.run("Who won the first cricket world cup?")


convo.run("How much is 5+5?")


convo.run("Who was the captain ofthe winning team?")


print(convo.memory.buffer)


from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(k=5)

convo = ConversationChain(
    llm=llm,
    memory=memory
)
convo.run("Who won the first cricket world cup?")


convo.run("How much is 5 + 5")


convo.run("Who was the captain of the winning team?")


print(convo.memory.buffer)


from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("/content/yolov7paper-1.pdf")
pages = loader.load()


pages












