{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a9f853-ff13-446c-bfaa-bebebf0d7d10",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLaMA 2 using a single GPU, QLoRA and AI Notebooks\n",
    "https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270742ba-f53b-4584-bcdd-72cfe8378dc3",
   "metadata": {},
   "source": [
    "## Mandatory requirements\n",
    "To successfully fine-tune LLaMA 2 models, you will need the following:\n",
    "\n",
    "Fill Meta’s form to request access to the next version of Llama. Indeed, the use of Llama 2 is governed by the Meta license, that you must accept in order to download the model weights and tokenizer.\n",
    "Have a Hugging Face account (with the same email address you entered in Meta’s form).\n",
    "Have a Hugging Face token.\n",
    "Visit the page of one of the LLaMA 2 available models (version 7B, 13B or 70B), and accept Hugging Face’s license terms and acceptable use policy.\n",
    "Log in to the Hugging Face model Hub from your notebook’s terminal by running the huggingface-cli login command, and enter your token. You will not need to add your token as git credential.\n",
    "Powerful Computing Resources: Fine-tuning the Llama 2 model requires substantial computational power. Ensure you are running code on GPU(s) when using AI Notebooks or AI Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f1168-69f2-459a-9d97-2c4d7e9025ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b698b45-050d-428f-ba22-cf2c1b465ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957f51d-a9a3-4da8-ac15-19c7c1b49ff8",
   "metadata": {},
   "source": [
    "### Download LLaMA 2 model\n",
    "To download the model you have been granted access to, make sure you are logged in to the Hugging Face model hub. As mentioned in the requirements step, you need to use the huggingface-cli login command.\n",
    "\n",
    "The following function will help us to download the model and its tokenizer. It requires a bitsandbytes configuration that we will define later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bd6c3-3213-4805-be2d-e96eda29678a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -q huggingface_hub\n",
    "#!huggingface-cli login\n",
    "\n",
    "# When prompted, paste the HF access token you created earlier.\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f098d999-d80b-43f0-9dbd-d6aed2a99a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16bbfb5-38a7-4741-9a20-a3e324f7864d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download a Dataset\n",
    "There are many datasets that can help you fine-tune your model. You can even use your own dataset!\n",
    "\n",
    "In this tutorial, we are going to download and use the Databricks Dolly 15k dataset, which contains 15,000 prompt/response pairs. It was crafted by over 5,000 Databricks employees during March and April of 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619c944-c34a-499b-9d81-cbe5f21efe4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the databricks dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')\n",
    "\n",
    "# *** OUTPUT ***\n",
    "# Number of prompts: 15011\n",
    "# Column Names are: ['instruction', 'context', 'response', 'category']\n",
    "#\n",
    "# As we can see, each sample is a dictionary that contains:\n",
    "#\n",
    "# An instruction: What could be entered by the user, such as a question\n",
    "# A context: Help to interpret the sample\n",
    "# A response: Answer to the instruction\n",
    "# A category: Classify the sample between Open Q&A, Closed Q&A, Extract information from Wikipedia, Summarize information from Wikipedia, Brainstorming, Classification, Creative writing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40856108-5154-4b73-a03a-969fa7d86e2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pre-processing dataset\n",
    "Instruction fine-tuning is a common technique used to fine-tune a base LLM for a specific downstream use-case.\n",
    "\n",
    "It will help us to format our prompts as follows\n",
    "\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a563d3cc-083c-4087-b1ca-8b4fb0720998",
   "metadata": {},
   "source": [
    "### Instruction:\n",
    "Sea or Mountain\n",
    "\n",
    "### Response:\n",
    "I believe Mountain are more attractive but Ocean has it's own beauty and this tropical weather definitely turn you on! SO 50% 50%\n",
    "\n",
    "### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c720c-d59e-43e4-bd9e-5653f85e8573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction', 'context', 'response')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    \n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304d036-823d-4e30-978b-185f101df1b4",
   "metadata": {},
   "source": [
    "Now, we will use our model tokenizer to process these prompts into tokenized ones.\n",
    "\n",
    "The goal is to create input sequences of uniform length (which are suitable for fine-tuning the language model because it maximizes efficiency and minimize computational overhead), that must not exceed the model’s maximum token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7cfa8-63bb-4d9b-9f32-ad5af2b808a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f26f10-2a8f-4283-bd5f-3e3d1d02767f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a bitsandbytes configuration\n",
    "This will allow us to load our LLM in 4 bits. This way, we can divide the used memory by 4 and import the model on smaller devices. We choose to apply bfloat16 compute data type and nested quantization for memory-saving purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a887d1-3c4a-4d8e-bcea-c682d67e034c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509dc81-dbe0-46d2-b355-a030b47b084d",
   "metadata": {},
   "source": [
    "To leverage the LoRa method, we need to wrap the model as a PeftModel.\n",
    "\n",
    "To do this, we need to implement a LoRa configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe95cd-78c6-4d74-95ea-7542b48f65d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26c606-cc51-4c47-b907-86e6d7744431",
   "metadata": {},
   "source": [
    "Previous function needs the target modules to update the necessary matrices. The following function will get them for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7efd5-12c4-4dad-8b95-963670fed5e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e764d-c991-40d2-9602-115a4666dd12",
   "metadata": {},
   "source": [
    "Once everything is set up and the base model is prepared, we can use the print_trainable_parameters() helper function to see how many trainable parameters are in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939be78-1459-41a4-91fc-2543794b5d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33324346-ef43-4f62-9a3e-f769081a8982",
   "metadata": {},
   "source": [
    "We expect the LoRa model to have fewer trainable parameters compared to the original one, since we want to perform fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98634375-d14b-495a-b144-41be8b0c8c4c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7ad13-124a-4403-85b6-88cd4b9886cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HF with user's token and with bitsandbytes config\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\" \n",
    "bnb_config = create_bnb_config()\n",
    "model, tokenizer = load_model(model_name, bnb_config)\n",
    "\n",
    "seed = 12321 \n",
    "\n",
    "## Preprocess dataset\n",
    "max_length = get_max_length(model)\n",
    "dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6961898d-6d2e-4e27-849c-e8d7b6bd6958",
   "metadata": {},
   "source": [
    "Then, we can run our fine-tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6488b1-089f-4322-8c35-fabbaad5f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            max_steps=20,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "     \n",
    "    do_train = True\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "output_dir = \"results/llama2/final_checkpoint\"\n",
    "train(model, tokenizer, dataset, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc98aee-b436-46ce-b548-abffa719de91",
   "metadata": {},
   "source": [
    "If you prefer to have a number of epochs (entire training dataset will be passed through the model) instead of a number of training steps (forward and backward passes through the model with one batch of data), you can replace the max_steps argument by num_train_epochs.\n",
    "\n",
    "Unfortunately, it is possible that the latest weights are not the best. To solve this problem, you can implement a EarlyStoppingCallback, from transformers, during your fine-tuning. This will enable you to regularly test your model on the validation set, if you have one, and keep only the best weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3905bf2-6515-4ffb-8f09-a54e2b88fc0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge weights\n",
    "\n",
    "Once we have our fine-tuned weights, we can build our fine-tuned model and save it to a new directory, with its associated tokenizer. By performing these steps, we can have a memory-efficient fine-tuned model and tokenizer ready for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095429f5-b249-40b6-b583-564a74d1c44d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = \"results/llama2/final_checkpoint\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = \"results/llama2/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\n",
    "# save tokenizer for easy inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da2a382-9b98-495a-9068-e5b2400f4459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload\n",
    "# model.push_to_hub(\"mychen76/llama-7b-dolly2\", create_pr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df39c797-5ef7-4e2d-8562-de775631eb2b",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d74384-12ae-4758-9bd3-0c339021016b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=trained_model_id)\n",
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(trained_model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(trained_model_id)\n",
    "\n",
    "trained_model = \"results/llama2/final_merged_checkpoint\"\n",
    "#model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = trained_model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8c48e-ebfa-4073-89cb-d1b643d3f174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
    "#!pip install xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d58acf-69eb-4720-8487-db93b5423d6c",
   "metadata": {},
   "source": [
    "## Test Client 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e06c18-044e-4999-9f76-304843bca441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "trained_model = \"results/llama2/final_merged_checkpoint\"\n",
    "#model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = trained_model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "# model     = AutoModel.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95edb017-f8a2-4282-838b-3fcd052863f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # pip install peft transformers\n",
    "# import torch\n",
    "# from peft import PeftModel, PeftConfig\n",
    "# from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "# from accelerate import infer_auto_device_map, init_empty_weights\n",
    "\n",
    "# peft_model_id = trained_model\n",
    "# config = PeftConfig.from_pretrained(peft_model_id)\n",
    "# model1 = LlamaForCausalLM.from_pretrained(\n",
    "#     config.base_model_name_or_path,\n",
    "#     torch_dtype='auto',\n",
    "#     device_map='auto',\n",
    "#     offload_folder=\"offload\", offload_state_dict = True\n",
    "# )\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# # Load the Lora model\n",
    "# model1 = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa97690a-0856-451e-911a-1470cea53d86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input_text=\"Hello, how are you?\"\n",
    "# inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "# outputs = model.generate(inputs,max_length=50, num_return_sequences=5,temperature=0.7)\n",
    "# print(\"Generated text:\")\n",
    "# for i, output in enumerate(outputs):\n",
    "#     print(f\"{i}:{tokenizer.decode(output)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
