{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5376d904-2c3e-4652-bdb2-24b1e43d9224",
   "metadata": {},
   "source": [
    "## Load and Test Fined-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2a7f2a-be61-490f-8d9a-33a6213f042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load the model meta-llama/Llama-2-7b-chat-hf into memory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a65cfa7314d45c98a8085b22cc75c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the model meta-llama/Llama-2-7b-chat-hf into memory\n"
     ]
    }
   ],
   "source": [
    "## properway to load fine-tuned models\n",
    "import torch\n",
    "from peft import PeftModel    \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "adapters_name = \"mychen76/Llama-2-7b-hf-guanaco-sm\"\n",
    "\n",
    "print(f\"Starting to load the model {model_name} into memory\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapters_name)\n",
    "model = model.merge_and_unload()\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.bos_token_id = 1\n",
    "stop_token_ids = [0]\n",
    "print(f\"Successfully loaded the model {model_name} into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc71a74-a6c3-48d0-96d6-277d9d3bc5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type {'': 0}. This is not supported.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Today was an amazing day because I got to go to the beach with my family! We had so much fun playing in the waves and building sandcastles.\\nI love spending time with my family because we always have so much fun together. We played games, ran around, and even had a sandcastle building contest. My little brother won, but I still had a great time.\\nThe best part of the day was when we found a really cool shell that my dad used to make a sandcastle. We']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Today was an amazing day because\"\n",
    "device_map={\"\": 0}\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device_map)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), do_sample=True, num_beams=1, max_new_tokens=100)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e69030f-b9dd-4e95-b8b2-0a9398cff21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Today was an amazing day',\n",
       " 'labels': ['positive', 'negative'],\n",
       " 'scores': [0.8899121284484863, 0.11008787900209427]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use it for zero-shot classification:\n",
    "from transformers import pipeline\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Today was an amazing day\", candidate_labels=[\"negative\", \"positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ede2e-7c58-4b06-84cc-3f00c2b38d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
